---
title: A Coordinate Descent Framework to Design Low PSL ISL Sequences 论文阅读
author: xinyu
date: 2023-06-21 13:56:37 +0800
math: true
categories: [科研记录]
tags: [论文阅读]
---

论文地址：[A Coordinate Descent Framework to Design Low PSL ISL Sequences](https://ieeexplore.ieee.org/document/7967829)

期刊/会议：IEEE TRANSACTIONS ON Signal Processing

发表时间：2017

## 1. 问题建模

在雷达系统的脉冲压缩过程中，通常希望具有低的峰值旁瓣电平（Peak Sidelobe Level, PSL）避免弱目标被强目标的旁瓣所遮蔽；同时，为了减轻相近目标的分布式杂波的回波的影响，我们需要信号具有低的积分旁瓣电平（Integrated Sidelobe Level, ISL）。考虑一个相位编码序列$\boldsymbol{x}=[x_1,x_2,...,x_N]^T\in \mathbb{C} ^N$，其自相关函数$r_k$、PSL、ISL的定义如下。

$$
r_k=\sum_{i=1}^{N-1}{x_{i}^{*}x_{i+k}}\quad k=0,1,...,N-1
$$

$$
PSL=max\left\{ \left| r_k \right| \right\} _{k=1}^{k=N-1}
$$

$$
ISL=\sum_{k=1}^{N-1}{\left| r_k \right|^2}
$$

此外，在雷达信号中，因需要保持最大的发射功率以获得最大的信噪比，故常常希望晶体管工作在饱和区、信号幅度始终为最大，即恒模约束。因此，针对于连续相位约束与离散相位约束，该问题可以表示为一个优化问题。

$$
P^{\infty}\lbrace \begin{matrix} \underset{\boldsymbol{x}}{\min}& f_1(\boldsymbol{x}),f_2(\boldsymbol{x})\\
s.t.& \boldsymbol{x}\in \Omega _{\infty}\\\end{matrix} ,P^M\lbrace \begin{matrix} \underset{\boldsymbol{x}}{\min}& f_1(\boldsymbol{x}),f_2(\boldsymbol{x})\\ s.t.& \boldsymbol{x}\in \Omega _M\\\end{matrix}  
$$

$$
f_1\left( \boldsymbol{x} \right) =\max \left\{ \left| r_k \right|^2 \right\} _{k=1}^{k=N-1}
$$

$$
f_2\left( \boldsymbol{x} \right) =\underset{k=1}{\overset{k=N-1}{\sum{\,}}}\left| r_k \right|^2
$$

$$
\Omega _{\infty}=\left\{ \boldsymbol{x}\in \mathbb{C} ^N\left| \, \right. \left| x_i \right|,i=1,...,N \right\} 
$$

$$
\Omega _M=\left\{ \boldsymbol{x}\left| \, \right. x_i\in \varPsi _M,i=1,...,N \right\} 
$$

$$
\varPsi _M=\{1,\bar{\omega},...,\bar{\omega}^{M-1}\},\bar{\omega}=e^{j\frac{2\pi}{M}}
$$

针对于多目标优化问题，通常不能寻找到对于所有目标函数都最优的一个解，已经被证明的是，对于多目标问题而言，通常存在一个帕累托最优解集（Pareto-optimal solutions），也可称为帕累托最优界；其表征着在这一界限上若想使某一个目标函数更优，则必然会导致其它目标函数的损失。针对于多目标优化问题，常用的方法有进化算法如粒子群算法、遗传算法、灰狼算法等，线性化技术也是常用于多目标优化的方法之一，即使用多个目标函数的凸组合代替原目标函数集，从而将多目标优化转化为一个单目标优化问题，在本问题中，由于两个目标函数的度量尺度相同，因此使用标量化技术是可行的措施，而标量化所需要的权系数则需要人为依据经验选定。因此目标函数可以转化为如下所示形式。

$$
P^{\infty ,\theta}\left\{ \begin{matrix}    \underset{\boldsymbol{x}}{\min}&        f_{\theta}(\boldsymbol{x})\\    s.t.&        \boldsymbol{x}\in \Omega _{\infty}\\\end{matrix} \right. ,P^{M,\theta}\left\{ \begin{matrix}    \underset{\boldsymbol{x}}{\min}&        f_{\theta}(\boldsymbol{x})\\    s.t.&        \boldsymbol{x}\in \Omega _M\\\end{matrix} \right. 
$$

$$
\begin{aligned}    
f_{\theta}(\boldsymbol{x})=&\,\,\theta f_1(\boldsymbol{x})+(1-\theta )f_2(\boldsymbol{x})\\    =&\max_{k=1,...,N-1} \left[ \theta |r_k|^2+(1-\theta )\sum_{l=1}^{N-1}{|r_l|^2} \right]\\
\end{aligned}
$$

## 2. 算法原理

### 2.1 坐标下降法

在恒模约束下，上式对应优化问题成为了一个非凸的、NP难的问题，在这里引入优化方法中常使用的坐标下降法对其进行处理，原本我们需要针对于一个维度为N的向量进行优化，坐标下降法即固定其中某一个维度，对其进行逐变量优化，如下所示：

$$
P_{d,\boldsymbol{x}^{(n)}}^{\infty ,\theta}\left\{ \begin{matrix}    \underset{x_d}{\min}&        f_{\theta}(x_d;\boldsymbol{x}_{-d}^{(n)})\\    s.t.&        \left| x_d \right|=1\\\end{matrix} \right. ,P_{d,\boldsymbol{x}^{(n)}}^{M,\theta}\left\{ \begin{matrix}    \underset{x_d}{\min}&        f_{\theta}(x_d;\boldsymbol{x}_{-d}^{(n)})\\    s.t.&        x_d\in \varPsi _M\\\end{matrix} \right. 
$$

$$
\boldsymbol{x}_{-d}^{(n)}=\left[ x_{1}^{(n)},...,x_{d-1}^{(n)},x_{d+1}^{(n)},...,x_{N}^{(n)} \right] ^T\in \mathbb{C} ^{N-1}
$$

$$
f_{\theta}(x_d;\boldsymbol{x}_{-d}^{(n)})=f_{\theta}(x_{1}^{(n)},...,x_{d-1}^{(n)},x_d,x_{d+1}^{(n)},...,x_{N}^{(n)}).
$$

针对于恒模约束下的坐标下降算法，文献表示使用最大块提升策略可以使其稳定的收敛到一个可行点，最大块提升策略即在逐变量优化的过程中只接受拥有最大块改进的块更新部分。在这里我们同样使用该策略进行更新。

对于连续相位情况，目标函数变为了单变量函数，因此针对于目标函数我们可以进行改写。

$$
\begin{aligned}    r_k(x_d)=&x_d(x_{d+k}^{(n)})^*1_A(d+k)+(x_{d-k}^{(n)})x_{d}^{*}1_A(d-k)\\    &+\sum_{i=1,i\ne \left\{ d,d-k \right\}}^{N-k}{x_{i}^{(n)}(x_{i+k}^{(n)})^*}, k=1,...,N-1,\\\end{aligned}
$$

$$
\mathbf{1}_A(x)=\left\{ \begin{aligned}    1\quad &\mathrm{if} \mathrm{x}\in A\\    0\quad &\mathrm{else}\\\end{aligned} \right. \quad A=\left\{ 1,2,...,N \right\} 
$$

注意到$r_k\left( x_d \right)$仅与$x_d$有关，我们定义$c_{dk}=\sum\nolimits_{i=1,i\ne \left\{ d,d-k \right\}}^{N-k}{x_{i}^{(n)}(x_{i+k}^{(n)})^*}$，$a_{dk}\triangleq (x_{d+k}^{(n)})^*1_A(d+k)$，$b_{dk}=(x_{d-k}^{(n)})1_A(d-k)$，故有：

$$
r_k(x_d)=a_{dk}x_d+b_{dk}x_{d}^{*}+c_{dk}
$$

因此有：

$$
P_{d,\boldsymbol{x}^{(n)}}^{\infty ,\theta}\left\{ \begin{matrix}    \underset{x_d}{\min}&        \max_{k=1,...,N-1} \left[ \theta \left| r_k(x_d) \right|^2+(1-\theta )\sum_{l=1}^{N-1}{\left| r_l(x_d) \right|^2} \right]\\    s.t.&        \left| x_d \right|=1\\\end{matrix} \right. \\P_{d,\boldsymbol{x}^{(n)}}^{M,\theta}\left\{ \begin{matrix}    \underset{x_d}{\min}&        \max_{k=1,...,N-1} \left[ \theta \left| r_k(x_d) \right|^2+(1-\theta )\sum_{l=1}^{N-1}{\left| r_l(x_d) \right|^2} \right]\\    s.t.&        x_d\in \varPsi _M\\\end{matrix} \right. 
$$

### 2.2 连续相位编码算法

### 2.3 离散相位编码算法
